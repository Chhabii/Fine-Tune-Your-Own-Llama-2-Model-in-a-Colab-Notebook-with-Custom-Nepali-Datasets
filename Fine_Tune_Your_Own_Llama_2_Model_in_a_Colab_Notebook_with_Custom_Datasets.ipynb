{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "In this section, we will fine-tune a Llama 2 model with 7 billion parameters on a T4 GPU with high RAM using Google Colab (2.21 credits/hour). Note that a T4 only has 16 GB of VRAM, which is barely enough to store Llama 2-7b’s weights (7b × 2 bytes = 14 GB in FP16). In addition, we need to consider the overhead due to optimizer states, gradients, and forward activations (see this excellent article for more information). This means that a full fine-tuning is not possible here: we need parameter-efficient fine-tuning (PEFT) techniques like LoRA or QLoRA.\n",
        "\n",
        "To drastically reduce the VRAM usage, we must fine-tune the model in 4-bit precision, which is why we’ll use QLoRA here. The good thing is that we can leverage the Hugging Face ecosystem with the transformers, accelerate, peft, trl, and bitsandbytes libraries.\n"
      ],
      "metadata": {
        "id": "cYUBBbgBPUgS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk_3TcRFO1YD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install transformers peft accelerate bitsandbytes trl datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging\n",
        ")\n",
        "from peft import LoraConfig,PeftModel\n",
        "from trl import SFTTrainer\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "-jz2nngHRGG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "\n",
        "nepali_dataset = \"Chhabi/testing-dataset-llama2-nepali-health\"\n",
        "\n",
        "new_model = \"testing1.1-llama2-nepali-health-model\"\n"
      ],
      "metadata": {
        "id": "OjQYmMONRH79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(nepali_dataset,split='train')\n"
      ],
      "metadata": {
        "id": "Xck0V7ZbRK7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_dtype = getattr(torch,\"float16\") #retrieves the attribute \"float16\" from the torch library and assigns it to compute_dtype.\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,#meaning the data will be loaded in 4-bit format.\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")"
      ],
      "metadata": {
        "id": "K4UY5q2vRNoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    quantization_config=quant_config,\n",
        "    device_map ={\"\":0},  # Argument sets the device mapping for the model, in this case, it's set to use the first GPU.\n",
        ")\n",
        "model.config.use_cache=False,#disables the use of cache in the model configuration\n",
        "model.config.pretraining_tp = 1 #sets the pretraining temperature parameter to 1 in the model configuration."
      ],
      "metadata": {
        "id": "Q_HXhUVGRPO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model,trust_remote_code=True,token=\"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\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side=\"right\""
      ],
      "metadata": {
        "id": "fmdM29IKRS0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_params = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,\n",
        "    bias='none',\n",
        "    task_type = \"CAUSAL_LM\"\n",
        ")"
      ],
      "metadata": {
        "id": "vDGgj03QRU8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = TrainingArguments(\n",
        "    output_dir=\"./results/nepali-health-llama-finetuning\",\n",
        "    num_train_epochs=1,  # Train for 3 epochs\n",
        "    per_device_train_batch_size=1,#increase if you've more ram else reduce\n",
        "    gradient_accumulation_steps=8,#decrease if you've more ram else increase the accumulation of gradients.\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=500,\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "\n",
        "    learning_rate=2e-4,  # Reduce learning rate due to smaller dataset\n",
        "    weight_decay=0.1,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03,  # Increase warmup ratio for smaller dataset\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    report_to=\"tensorboard\"\n",
        ")"
      ],
      "metadata": {
        "id": "cGPAkJ-ERWeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    peft_config=peft_params,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=512, ################## keep None if you've more RAM else reduce it.\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_params,\n",
        "    packing=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "la8AYRJ0RYFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "HuZGxTAVRnrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir results/nepali-health-llama-finetuning/runs"
      ],
      "metadata": {
        "id": "_9EW0FWaRsiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.save_pretrained(new_model)\n"
      ],
      "metadata": {
        "id": "kdrblEEqRtTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"के शिशुहरूमा जटिल जन्मजात हृदय रोगको स्थायी समाधान छ? यो अवस्था भएको 8 महिनाको बच्चाको आहार कस्तो हुनुपर्छ?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
        "print(result[0]['generated_text'])"
      ],
      "metadata": {
        "id": "SA-f0PApRv82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMPORTANT:\n",
        "after the training is completed, you've to copy the adapter models to google drive as\n",
        "```\n",
        "cp -r /content/testing1.1-llama2-nepali-health-model /content/drive/MyDrive/ColabFolder/\n",
        "```\n",
        "Restart the session, copy the adapter model from google drive to colab.\n",
        "\n",
        "```\n",
        "cp -r /content/drive/MyDrive/ColabFolder/testing1.1-llama2-nepali-health-model/  /content/\n",
        "```\n"
      ],
      "metadata": {
        "id": "BWt7W8FHR0zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device_map = {\"\": 0}\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=device_map,\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, new_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ],
      "metadata": {
        "id": "cCYgZONWRz5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(new_model, use_temp_dir=False)\n",
        "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
      ],
      "metadata": {
        "id": "G-5WHkoeTiJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model is saved to the huggingface, now for inference"
      ],
      "metadata": {
        "id": "SmiZPWyHTpe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install transformers peft accelerate trl datasets\n",
        "%pip install -i https://pypi.org/simple/ bitsandbytes --upgrade\n"
      ],
      "metadata": {
        "id": "qS5iHWUGTofM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_name = \"Chhabi/testing1.1-llama2-nepali-health-model\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,torch_dtype=torch.float32)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=512,\n",
        "    torch_dtype = torch.float32\n",
        ")\n",
        "\n",
        "# Input query\n",
        "prompt = \"मलाई सडकको कुकुरले मेरो हातको औंलाको छेउमा टोकेको थियो। खरोंच थियो र रगत निस्कियो। घाउ 10 घण्टा पछि निको भयो। मैले पहिलो दिन डाक्टरलाई भेटिन, तर केही सल्लाह पछि, मैले सुरक्षाको लागि खोप लगाउने निर्णय गरें। मैले टोकेको करिब ५० घण्टापछि तेस्रो दिन रबिपुर र टीटी खोप लिएँ। अब मैले थप ४ वटा सुई लगाउनु पर्छ । म सुरक्षित छु वा ढिलाइ धेरै लामो थियो?\"\n",
        "\n",
        "# Generate text based on the input query\n",
        "generated_text = text_generation_pipeline(f\"<s>[INST] {prompt} [/INST]\")[0]['generated_text']\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "8N_9o49gTocY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}